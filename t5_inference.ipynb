{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee25e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA Device Count: 1\n",
      "Device 0: Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45c2a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:14: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:64: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de61028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./t5-finetuned-squad-custom\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdbb20f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-t5/t5-base\"\n",
    "proxy = \"http://sisproxy.hkg.agoda.local:3128\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, proxies={\"http\": proxy, \"https\": proxy})\n",
    "print('Tokenizer loaded successfully')\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, proxies={\"http\": proxy, \"https\": proxy}).to(device)\n",
    "print('Model loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6bc65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10570/10570 [18:48<00:00,  9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_squad(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    \n",
    "    dataset = []\n",
    "    for article in squad_dict[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                qid = qa[\"id\"]\n",
    "                dataset.append({\"id\": qid, \"context\": context, \"question\": question})\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = load_squad(\"dev-v1.1.json\")\n",
    "print('Dataset loaded successfully')\n",
    "\n",
    "def generate_answer(context, question):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids.to(device)\n",
    "    outputs = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "predictions = {}\n",
    "for example in tqdm(dataset):\n",
    "    question = example['question']\n",
    "    context = example['context']\n",
    "    qid = example['id']\n",
    "    \n",
    "    answer = generate_answer(context, question)\n",
    "    predictions[qid] = answer\n",
    "\n",
    "with open('predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "print(\"Predictions saved to predictions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe162c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"exact\": 80.54872280037843,\r\n",
      "  \"f1\": 89.39400002730301,\r\n",
      "  \"total\": 10570,\r\n",
      "  \"HasAns_exact\": 80.54872280037843,\r\n",
      "  \"HasAns_f1\": 89.39400002730301,\r\n",
      "  \"HasAns_total\": 10570\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate-v2.0.py dev-v1.1.json predictions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee196d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = \"google-t5/t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "print('Tokenizer loaded successfully')\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "print('Model loaded successfully')\n",
    "def load_squad(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    \n",
    "    dataset = []\n",
    "    for article in squad_dict[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                qid = qa[\"id\"]\n",
    "                dataset.append({\"id\": qid, \"context\": context, \"question\": question})\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = load_squad(\"dev-v1.1.json\")\n",
    "print('Dataset loaded successfully')\n",
    "\n",
    "def generate_answer(context, question):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids.to(device)\n",
    "    outputs = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "predictions = {}\n",
    "for example in tqdm(dataset):\n",
    "    question = example['question']\n",
    "    context = example['context']\n",
    "    qid = example['id']\n",
    "    \n",
    "    answer = generate_answer(context, question)\n",
    "    predictions[qid] = answer\n",
    "\n",
    "with open('predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "print(\"Predictions saved to predictions.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
