{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d906afe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen model...\n",
      "Loading predictions and SQuAD data...\n",
      "Generating ensemble predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10570/10570 [48:53<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions...\n",
      "\n",
      "Sample predictions:\n",
      "\n",
      "Question ID: 56be4db0acb8001400a502ec\n",
      "Prediction: Denver Broncos\n",
      "\n",
      "Question ID: 56be4db0acb8001400a502ed\n",
      "Prediction: Carolina Panthers\n",
      "\n",
      "Question ID: 56be4db0acb8001400a502ee\n",
      "Prediction: Levi's Stadium in the San Francisco Bay Area\n",
      "\n",
      "GPU Memory Usage:\n",
      "Allocated: 2956.16 MB\n",
      "Reserved: 6714.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "import time\n",
    "\n",
    "# Device setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "proxy = \"http://sisproxy.hkg.agoda.local:3128\"\n",
    "proxy_config = {\"http\": proxy, \"https\": proxy}\n",
    "\n",
    "def load_qwen():\n",
    "    \"\"\"Load Qwen model and tokenizer\"\"\"\n",
    "    model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        proxies=proxy_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        proxies=proxy_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def load_predictions_and_squad(prediction_files: List[str], squad_file: str) -> Dict:\n",
    "    \"\"\"Load predictions from multiple files and original SQuAD data\"\"\"\n",
    "    # Load predictions\n",
    "    predictions_by_id = {}\n",
    "    model_names = [\"Model1 (LLaMA)\", \"Model2 (T5)\", \"Model3 (Gemma)\"]\n",
    "    \n",
    "    for file_path, model_name in zip(prediction_files, model_names):\n",
    "        with open(file_path, 'r') as f:\n",
    "            predictions = json.load(f)\n",
    "            for qid, answer in predictions.items():\n",
    "                if qid not in predictions_by_id:\n",
    "                    predictions_by_id[qid] = []\n",
    "                predictions_by_id[qid].append((model_name, answer))\n",
    "    \n",
    "    # Load SQuAD data\n",
    "    with open(squad_file, 'r') as f:\n",
    "        squad_data = json.load(f)\n",
    "    \n",
    "    # Create mapping of question IDs to questions and contexts\n",
    "    qa_info = {}\n",
    "    for article in squad_data['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                qa_info[qa['id']] = {\n",
    "                    'question': qa['question'],\n",
    "                    'context': context\n",
    "                }\n",
    "    \n",
    "    return predictions_by_id, qa_info\n",
    "\n",
    "def get_ensemble_prediction(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    question: str, \n",
    "    context: str, \n",
    "    predictions: List[tuple]\n",
    ") -> str:\n",
    "    \"\"\"Get ensemble prediction using Qwen\"\"\"\n",
    "    # Format predictions string\n",
    "    pred_str = \"\\n\".join([f\"{name}: {pred}\" for name, pred in predictions])\n",
    "    \n",
    "    # Create messages for chat template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert at reading comprehension and answer analysis. Your task is to determine the most accurate answer based on the given context and model predictions, but feel free to give other answer if you think none of the given predictions are correct.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Different model predictions:\n",
    "{pred_str}\n",
    "\n",
    "Based on the context and these predictions, what is the most accurate answer? Only output the exact answer text, no explanations.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    # Extract only the new tokens\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids \n",
    "        in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "def create_ensemble_predictions(squad_file: str, prediction_files: List[str], output_file: str):\n",
    "    \"\"\"Create ensemble predictions for SQuAD dataset\"\"\"\n",
    "    print(\"Loading Qwen model...\")\n",
    "    model, tokenizer = load_qwen()\n",
    "    \n",
    "    print(\"Loading predictions and SQuAD data...\")\n",
    "    predictions_by_id, qa_info = load_predictions_and_squad(prediction_files, squad_file)\n",
    "    \n",
    "    print(\"Generating ensemble predictions...\")\n",
    "    final_predictions = {}\n",
    "    \n",
    "    for qid, predictions in tqdm(predictions_by_id.items()):\n",
    "        if qid not in qa_info:\n",
    "            continue\n",
    "            \n",
    "        question = qa_info[qid]['question']\n",
    "        context = qa_info[qid]['context']\n",
    "        \n",
    "        ensemble_prediction = get_ensemble_prediction(\n",
    "            model, \n",
    "            tokenizer,\n",
    "            question,\n",
    "            context,\n",
    "            predictions\n",
    "        )\n",
    "        \n",
    "        final_predictions[qid] = ensemble_prediction\n",
    "    \n",
    "    print(\"Saving predictions...\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(final_predictions, f, indent=2)\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    squad_file = \"dev-v1.1.json\"\n",
    "    prediction_files = [\n",
    "        \"predictions_llama.json\",\n",
    "        \"t5_base_predictions.json\",\n",
    "        \"dev-v1.1-gemma-it-qa-3-processed.json\"\n",
    "    ]\n",
    "    output_file = \"qwen_ensemble_predictions.json\"\n",
    "    \n",
    "    try:\n",
    "        # Create ensemble predictions\n",
    "        predictions = create_ensemble_predictions(squad_file, prediction_files, output_file)\n",
    "        \n",
    "        # Print sample predictions\n",
    "        print(\"\\nSample predictions:\")\n",
    "        for qid in list(predictions.keys())[:3]:\n",
    "            print(f\"\\nQuestion ID: {qid}\")\n",
    "            print(f\"Prediction: {predictions[qid]}\")\n",
    "        \n",
    "        # Print memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            memory_allocated = torch.cuda.memory_allocated(0) / 1024**2\n",
    "            memory_reserved = torch.cuda.memory_reserved(0) / 1024**2\n",
    "            print(f\"\\nGPU Memory Usage:\")\n",
    "            print(f\"Allocated: {memory_allocated:.2f} MB\")\n",
    "            print(f\"Reserved: {memory_reserved:.2f} MB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:14: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:64: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions and SQuAD data...\n",
      "Generating ensemble predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████████████████████████████████████████████████████████████████████████▎                                                                                                           | 4704/10570 [25:15<31:55,  3.06it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "import time\n",
    "\n",
    "# Device setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "proxy = \"http://sisproxy.hkg.agoda.local:3128\"\n",
    "proxy_config = {\"http\": proxy, \"https\": proxy}\n",
    "\n",
    "def load_qwen():\n",
    "    \"\"\"Load Qwen model and tokenizer\"\"\"\n",
    "    model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        proxies=proxy_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        proxies=proxy_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def load_predictions_and_squad(prediction_files: List[str], squad_file: str) -> Dict:\n",
    "    \"\"\"Load predictions from multiple files and original SQuAD data\"\"\"\n",
    "    # Load predictions\n",
    "    predictions_by_id = {}\n",
    "    model_names = [\"LLaMA-3.2B\", \"Phi-3.5\"]\n",
    "    \n",
    "    for file_path, model_name in zip(prediction_files, model_names):\n",
    "        with open(file_path, 'r') as f:\n",
    "            predictions = json.load(f)\n",
    "            for qid, result in predictions.items():\n",
    "                if qid not in predictions_by_id:\n",
    "                    predictions_by_id[qid] = []\n",
    "                predictions_by_id[qid].append((model_name, result['res']))\n",
    "    \n",
    "    # Load SQuAD data\n",
    "    with open(squad_file, 'r') as f:\n",
    "        squad_data = json.load(f)\n",
    "    \n",
    "    # Create mapping of question IDs to questions and contexts\n",
    "    qa_info = {}\n",
    "    for article in squad_data['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                qa_info[qa['id']] = {\n",
    "                    'question': qa['question'],\n",
    "                    'context': context\n",
    "                }\n",
    "    \n",
    "    return predictions_by_id, qa_info\n",
    "\n",
    "def get_ensemble_prediction(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    question: str, \n",
    "    context: str, \n",
    "    predictions: List[tuple]\n",
    ") -> str:\n",
    "    \"\"\"Get ensemble prediction using Qwen\"\"\"\n",
    "    # Format predictions string\n",
    "    pred_str = \"\\n\".join([f\"{name}: {pred}\" for name, pred in predictions])\n",
    "    \n",
    "    # Create messages for chat template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert at reading comprehension and answer analysis. Your task is to determine the most accurate answer based on the given context, model predictions, and their explanations. Each model provides both an answer and its reasoning. If none of the predictions are satisfactory, you should provide your own answer based on the context.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Different model predictions (including their answers and explanations):\n",
    "{pred_str}\n",
    "\n",
    "Based on the context and the model predictions (considering both their answers and explanations), what is the most accurate answer? Only output the exact answer text, no explanations.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    # Extract only the new tokens\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids \n",
    "        in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "def create_ensemble_predictions(squad_file: str, prediction_files: List[str], output_file: str):\n",
    "    \"\"\"Create ensemble predictions for SQuAD dataset\"\"\"\n",
    "    print(\"Loading Qwen model...\")\n",
    "    model, tokenizer = load_qwen()\n",
    "    \n",
    "    print(\"Loading predictions and SQuAD data...\")\n",
    "    predictions_by_id, qa_info = load_predictions_and_squad(prediction_files, squad_file)\n",
    "    \n",
    "    print(\"Generating ensemble predictions...\")\n",
    "    final_predictions = {}\n",
    "    \n",
    "    for qid, predictions in tqdm(predictions_by_id.items()):\n",
    "        if qid not in qa_info:\n",
    "            continue\n",
    "            \n",
    "        question = qa_info[qid]['question']\n",
    "        context = qa_info[qid]['context']\n",
    "        \n",
    "        ensemble_prediction = get_ensemble_prediction(\n",
    "            model, \n",
    "            tokenizer,\n",
    "            question,\n",
    "            context,\n",
    "            predictions\n",
    "        )\n",
    "        \n",
    "        final_predictions[qid] = ensemble_prediction\n",
    "    \n",
    "    print(\"Saving predictions...\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(final_predictions, f, indent=2)\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    squad_file = \"dev-v1.1.json\"\n",
    "    prediction_files = [\n",
    "        \"llama-3.2-3b-explanation-tuned.json\",\n",
    "        \"phi-3.5-mini-instruct-explanation-tuned.json\"\n",
    "    ]\n",
    "    output_file = \"qwen_ensemble_predictions_2.json\"\n",
    "    \n",
    "    try:\n",
    "        # Create ensemble predictions\n",
    "        predictions = create_ensemble_predictions(squad_file, prediction_files, output_file)\n",
    "        \n",
    "        # Print sample predictions\n",
    "        print(\"\\nSample predictions:\")\n",
    "        for qid in list(predictions.keys())[:3]:\n",
    "            print(f\"\\nQuestion ID: {qid}\")\n",
    "            print(f\"Prediction: {predictions[qid]}\")\n",
    "        \n",
    "        # Print memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            memory_allocated = torch.cuda.memory_allocated(0) / 1024**2\n",
    "            memory_reserved = torch.cuda.memory_reserved(0) / 1024**2\n",
    "            print(f\"\\nGPU Memory Usage:\")\n",
    "            print(f\"Allocated: {memory_allocated:.2f} MB\")\n",
    "            print(f\"Reserved: {memory_reserved:.2f} MB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428c60ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"exact\": 78.81740775780511,\r\n",
      "  \"f1\": 88.60037451520424,\r\n",
      "  \"total\": 10570,\r\n",
      "  \"HasAns_exact\": 78.81740775780511,\r\n",
      "  \"HasAns_f1\": 88.60037451520424,\r\n",
      "  \"HasAns_total\": 10570\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate-v2.0.py dev-v1.1.json qwen_ensemble_predictions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7b7753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"exact\": 80.54872280037843,\r\n",
      "  \"f1\": 89.39400002730301,\r\n",
      "  \"total\": 10570,\r\n",
      "  \"HasAns_exact\": 80.54872280037843,\r\n",
      "  \"HasAns_f1\": 89.39400002730301,\r\n",
      "  \"HasAns_total\": 10570\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate-v2.0.py dev-v1.1.json t5_base_predictions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023615a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
